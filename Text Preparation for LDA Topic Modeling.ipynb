{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preparation for LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction:\n",
    "The purpose for text pre-processing is to make the text ready for further analysis. Text pre-processing could include many steps depending upon the type of data and the business problem. In our case, we have broken down text pre-processing into three different steps: pre-processing, lemmatization and noun extraction. In the pre-processing, all the text in the corpus was first lowercased so as to avoid algorithm reading 'Environment' and 'environment' as two different words. Next, the regular expressions were used to eliminate duplicate whitespaces, remove special characters, numbers and words which were less than three character length followed by the removal of stopwords. In the lemmatizer step, words were replaced with their root form using spaCy lemmatizer. For example: the lemma of the word 'plants' is 'plant'. Likewise, 'classifying'-> 'classify'. In the noun extraction step, only the nouns were extracted and other parts of speech were ignored as nouns are more indicative of the topic of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective: \n",
    "Making the text ready to be used for topic modeling with only the words of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input: \n",
    "The input for text pre-processing is the corpus of the documents of interest i.e. .txt files converted from their pdf version (for eg. ESA documents in our analysis) saved on a local directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output:\n",
    "The output after text pre-processing is a folder in Python's working directory which would have .txt files for each pdf document with the resulting noun words in their lemmatized root form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Packages Used:\n",
    "1. NLTK (for removing stopwords)\n",
    "2. spaCy (for Lemmatization and noun extraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import codecs\n",
    "import operator\n",
    "import csv\n",
    "import tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "nlp.max_length = 6000000 #spaCy cannot process more than 1 million characters at once. Therefore nlp.max.length has to be changed as per the length of the text fed into the spaCy library functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(mystr):\n",
    "    my_new_str = re.sub(\"(\\\\W| +)\",\" \", mystr) #remove anything that is not a letter or number\n",
    "    my_new_str = re.sub(r'\\s+', ' ', my_new_str) #eliminate duplicate whitespaces\n",
    "    my_new_str = re.sub(r\"\\b\\d+\\b\", \"\", my_new_str)\n",
    "    my_new_str = re.sub(\"\\d+\", \"\", my_new_str) #remove numbers from a string\n",
    "    my_new_str = my_new_str.replace('Ã©', 'e')\n",
    "    my_new_str = re.sub(r\"[^a-zA-Z0-9]+\",' ', my_new_str) #remove special characters\n",
    "    my_new_str = re.sub(r'\\b\\w{1,2}\\b', '', my_new_str) #remove words of length less than 3 from string\n",
    "    my_new_str = re.sub(r'\\b(' + r'|'.join(stopwords.words('english')) + r')\\b\\s*','', my_new_str) #remove stopwords\n",
    "    my_new_str = my_new_str.strip()\n",
    "    my_new_str = re.sub(' +', ' ', my_new_str)\n",
    "    return my_new_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### spaCy Lemmatization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmaspacy(my_new_str):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    nlp.max_length = 6000000\n",
    "    sentence = my_new_str\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc]) # joining all the word tokens after lemmatizer implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Files and Initializing Functions clean(), lemmaspacy() and Extracting Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_documents = [] #creating empty list to append entire text of the corpus in this list\n",
    "file_names = [] #creating empty list to append name of ESA .txt files \n",
    "#all_tokens = []\n",
    "#all_lemmatized_tokens = []\n",
    "#all_lemmatized_tokens_nouns = []\n",
    "files = os.listdir(\"X:/xxxx/xxxx/xxxxx/\") #directory path where ESAs as text files are saved\n",
    "for file in files:\n",
    "    with codecs.open(\"X:/xxxx/xxxx/xxxx/\" + file,'r', encoding='utf-8') as corpus: #directory path where ESAs as text files are saved\n",
    "        file_names.append(file) #appending file names\n",
    "        input_str = corpus.read().lower() #lowercasing all text in the corpus\n",
    "        #print(len(input_str.split()), 'read')\n",
    "        #print(input_str.split())\n",
    "        input_str = clean(input_str) #calling function clean()\n",
    "        #all_tokens.extend(input_str.split())\n",
    "        #print(len(input_str.split()), 'clean')\n",
    "        input_str = lemmaspacy(input_str) #calling function lemmaspacy()\n",
    "        #print(len(input_str.split()), 'lemmawordnet')\n",
    "        #all_lemmatized_tokens.extend(input_str.split())\n",
    "        list_element= \"\"\n",
    "        input_str = nlp(input_str)\n",
    "        #print(type(input_str), 'nlp')\n",
    "        #noun_checker = []\n",
    "        for chunk in input_str.noun_chunks: #for loop for noun extraction\n",
    "            list_element = list_element +\" \"+chunk.text #appending nouns separated by a space\n",
    "            #noun_checker.extend(str(chunk.text).split())\n",
    "        #print(len(noun_checker), 'nouns')\n",
    "        #all_lemmatized_tokens_nouns.extend(noun_checker)\n",
    "        list_documents.append(list_element) #appending all the nouns in the list called list_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing all the pre-processed, lemmatized and noun extracted .txt files in Python's Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_documents)):\n",
    "    with open(file_names[i], 'w') as f:\n",
    "        f.write(list_documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix (other lemmatization approaches that were tried)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Wordnet Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmawordnet(my_new_str):\n",
    "    sentence_words = nltk.word_tokenize(my_new_str)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(word, pos='n') for word in sentence_words]) \n",
    "    return lemmatized_output\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code for Textblob Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from textblob import TextBlob, Word\n",
    "def lemmatextblob(my_new_str):\n",
    "    sentence = my_new_str\n",
    "    sent = TextBlob(sentence)\n",
    "    tag_dict = {\"J\": 'a', \n",
    "                \"N\": 'n', \n",
    "                \"V\": 'v', \n",
    "                \"R\": 'r'}\n",
    "    words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]    \n",
    "    lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
    "    return \" \".join(lemmatized_list)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
